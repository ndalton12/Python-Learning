{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(\"images\", fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "\n",
    "Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\", kernel_initializer=he_init,\n",
    "            activation=tf.nn.elu)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, name=\"hidden2\", kernel_initializer=he_init,\n",
    "            activation=tf.nn.elu)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, name=\"hidden3\", kernel_initializer=he_init,\n",
    "            activation=tf.nn.elu)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "    \n",
    "    hidden4 = tf.layers.dense(hidden3_drop, n_hidden4, name=\"hidden4\", kernel_initializer=he_init,\n",
    "            activation=tf.nn.elu)\n",
    "    hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)\n",
    "    \n",
    "    hidden5 = tf.layers.dense(hidden4_drop, n_hidden5, name=\"hidden5\", kernel_initializer=he_init,\n",
    "            activation=tf.nn.elu)\n",
    "    hidden5_drop = tf.layers.dropout(hidden5, dropout_rate, training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(hidden5_drop, n_outputs, name=\"outputs\", kernel_initializer=he_init)\n",
    "    \n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.158070\tBest loss: 0.158070\tAccuracy: 95.00%\n",
      "1\tValidation loss: 0.119034\tBest loss: 0.119034\tAccuracy: 96.13%\n",
      "2\tValidation loss: 0.106546\tBest loss: 0.106546\tAccuracy: 96.91%\n",
      "3\tValidation loss: 0.112165\tBest loss: 0.106546\tAccuracy: 96.64%\n",
      "4\tValidation loss: 0.098725\tBest loss: 0.098725\tAccuracy: 97.26%\n",
      "5\tValidation loss: 0.091709\tBest loss: 0.091709\tAccuracy: 97.58%\n",
      "6\tValidation loss: 0.083497\tBest loss: 0.083497\tAccuracy: 97.65%\n",
      "7\tValidation loss: 0.069394\tBest loss: 0.069394\tAccuracy: 98.05%\n",
      "8\tValidation loss: 0.076006\tBest loss: 0.069394\tAccuracy: 97.97%\n",
      "9\tValidation loss: 0.067357\tBest loss: 0.067357\tAccuracy: 98.32%\n",
      "10\tValidation loss: 0.067301\tBest loss: 0.067301\tAccuracy: 98.08%\n",
      "11\tValidation loss: 0.060715\tBest loss: 0.060715\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.060734\tBest loss: 0.060715\tAccuracy: 98.51%\n",
      "13\tValidation loss: 0.058280\tBest loss: 0.058280\tAccuracy: 98.36%\n",
      "14\tValidation loss: 0.057121\tBest loss: 0.057121\tAccuracy: 98.40%\n",
      "15\tValidation loss: 0.054542\tBest loss: 0.054542\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.056648\tBest loss: 0.054542\tAccuracy: 98.51%\n",
      "17\tValidation loss: 0.054106\tBest loss: 0.054106\tAccuracy: 98.51%\n",
      "18\tValidation loss: 0.054326\tBest loss: 0.054106\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.054459\tBest loss: 0.054106\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.054193\tBest loss: 0.054106\tAccuracy: 98.51%\n",
      "21\tValidation loss: 0.057153\tBest loss: 0.054106\tAccuracy: 98.51%\n",
      "22\tValidation loss: 0.049697\tBest loss: 0.049697\tAccuracy: 98.44%\n",
      "23\tValidation loss: 0.050122\tBest loss: 0.049697\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.050942\tBest loss: 0.049697\tAccuracy: 98.79%\n",
      "25\tValidation loss: 0.046851\tBest loss: 0.046851\tAccuracy: 98.67%\n",
      "26\tValidation loss: 0.052448\tBest loss: 0.046851\tAccuracy: 98.63%\n",
      "27\tValidation loss: 0.048087\tBest loss: 0.046851\tAccuracy: 98.71%\n",
      "28\tValidation loss: 0.055219\tBest loss: 0.046851\tAccuracy: 98.55%\n",
      "29\tValidation loss: 0.047016\tBest loss: 0.046851\tAccuracy: 98.75%\n",
      "30\tValidation loss: 0.045860\tBest loss: 0.045860\tAccuracy: 98.91%\n",
      "31\tValidation loss: 0.043819\tBest loss: 0.043819\tAccuracy: 98.71%\n",
      "32\tValidation loss: 0.048497\tBest loss: 0.043819\tAccuracy: 98.75%\n",
      "33\tValidation loss: 0.045888\tBest loss: 0.043819\tAccuracy: 98.75%\n",
      "34\tValidation loss: 0.045734\tBest loss: 0.043819\tAccuracy: 98.79%\n",
      "35\tValidation loss: 0.048359\tBest loss: 0.043819\tAccuracy: 98.71%\n",
      "36\tValidation loss: 0.047420\tBest loss: 0.043819\tAccuracy: 98.75%\n",
      "37\tValidation loss: 0.043140\tBest loss: 0.043140\tAccuracy: 99.02%\n",
      "38\tValidation loss: 0.048298\tBest loss: 0.043140\tAccuracy: 98.83%\n",
      "39\tValidation loss: 0.043139\tBest loss: 0.043139\tAccuracy: 98.91%\n",
      "40\tValidation loss: 0.039555\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "41\tValidation loss: 0.042715\tBest loss: 0.039555\tAccuracy: 98.87%\n",
      "42\tValidation loss: 0.045266\tBest loss: 0.039555\tAccuracy: 98.83%\n",
      "43\tValidation loss: 0.041075\tBest loss: 0.039555\tAccuracy: 98.98%\n",
      "44\tValidation loss: 0.043365\tBest loss: 0.039555\tAccuracy: 98.83%\n",
      "45\tValidation loss: 0.044863\tBest loss: 0.039555\tAccuracy: 98.83%\n",
      "46\tValidation loss: 0.045313\tBest loss: 0.039555\tAccuracy: 98.87%\n",
      "47\tValidation loss: 0.041369\tBest loss: 0.039555\tAccuracy: 99.02%\n",
      "48\tValidation loss: 0.042899\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "49\tValidation loss: 0.045162\tBest loss: 0.039555\tAccuracy: 98.79%\n",
      "50\tValidation loss: 0.043853\tBest loss: 0.039555\tAccuracy: 98.79%\n",
      "51\tValidation loss: 0.045150\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "52\tValidation loss: 0.045923\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "53\tValidation loss: 0.042442\tBest loss: 0.039555\tAccuracy: 98.75%\n",
      "54\tValidation loss: 0.047704\tBest loss: 0.039555\tAccuracy: 98.91%\n",
      "55\tValidation loss: 0.044267\tBest loss: 0.039555\tAccuracy: 98.83%\n",
      "56\tValidation loss: 0.042481\tBest loss: 0.039555\tAccuracy: 98.71%\n",
      "57\tValidation loss: 0.039936\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "58\tValidation loss: 0.041619\tBest loss: 0.039555\tAccuracy: 98.94%\n",
      "59\tValidation loss: 0.038341\tBest loss: 0.038341\tAccuracy: 98.91%\n",
      "60\tValidation loss: 0.040457\tBest loss: 0.038341\tAccuracy: 98.91%\n",
      "61\tValidation loss: 0.040065\tBest loss: 0.038341\tAccuracy: 98.87%\n",
      "62\tValidation loss: 0.040760\tBest loss: 0.038341\tAccuracy: 98.71%\n",
      "63\tValidation loss: 0.046134\tBest loss: 0.038341\tAccuracy: 98.71%\n",
      "64\tValidation loss: 0.040317\tBest loss: 0.038341\tAccuracy: 98.79%\n",
      "65\tValidation loss: 0.041611\tBest loss: 0.038341\tAccuracy: 98.79%\n",
      "66\tValidation loss: 0.043151\tBest loss: 0.038341\tAccuracy: 98.91%\n",
      "67\tValidation loss: 0.036663\tBest loss: 0.036663\tAccuracy: 99.02%\n",
      "68\tValidation loss: 0.039170\tBest loss: 0.036663\tAccuracy: 98.87%\n",
      "69\tValidation loss: 0.039402\tBest loss: 0.036663\tAccuracy: 98.91%\n",
      "70\tValidation loss: 0.043791\tBest loss: 0.036663\tAccuracy: 98.83%\n",
      "71\tValidation loss: 0.038422\tBest loss: 0.036663\tAccuracy: 98.94%\n",
      "72\tValidation loss: 0.039595\tBest loss: 0.036663\tAccuracy: 98.91%\n",
      "73\tValidation loss: 0.037706\tBest loss: 0.036663\tAccuracy: 99.02%\n",
      "74\tValidation loss: 0.042495\tBest loss: 0.036663\tAccuracy: 98.83%\n",
      "75\tValidation loss: 0.039362\tBest loss: 0.036663\tAccuracy: 98.79%\n",
      "76\tValidation loss: 0.037499\tBest loss: 0.036663\tAccuracy: 98.94%\n",
      "77\tValidation loss: 0.036931\tBest loss: 0.036663\tAccuracy: 98.91%\n",
      "78\tValidation loss: 0.038721\tBest loss: 0.036663\tAccuracy: 99.06%\n",
      "79\tValidation loss: 0.042871\tBest loss: 0.036663\tAccuracy: 98.94%\n",
      "80\tValidation loss: 0.040367\tBest loss: 0.036663\tAccuracy: 98.87%\n",
      "81\tValidation loss: 0.037989\tBest loss: 0.036663\tAccuracy: 98.75%\n",
      "82\tValidation loss: 0.037876\tBest loss: 0.036663\tAccuracy: 98.94%\n",
      "83\tValidation loss: 0.040226\tBest loss: 0.036663\tAccuracy: 98.71%\n",
      "84\tValidation loss: 0.038939\tBest loss: 0.036663\tAccuracy: 98.91%\n",
      "85\tValidation loss: 0.038533\tBest loss: 0.036663\tAccuracy: 98.94%\n",
      "86\tValidation loss: 0.040005\tBest loss: 0.036663\tAccuracy: 98.87%\n",
      "87\tValidation loss: 0.036793\tBest loss: 0.036663\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 99.16%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1, training:False})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Transfer learning\n",
    "\n",
    "Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one.\n",
    "\n",
    "Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "\n",
    "Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "\n",
    "Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "\n",
    "Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'accuracy:0' refers to a Tensor which does not exist. The operation, 'accuracy', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ae18d860d1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mY_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y_proba:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2731\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\"\n\u001b[1;32m   2732\u001b[0m                       % type(name).__name__)\n\u001b[0;32m-> 2733\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2624\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   2625\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2626\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   2627\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'accuracy:0' refers to a Tensor which does not exist. The operation, 'accuracy', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_mnist_model_0_to_4.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
